{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import mdtraj\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '/var/home/vs488/Documents/boltzmann/code/boltzmann-generators/')\n",
    "import boltzgen.zmatrix as zmatrix\n",
    "import boltzgen.internal as ics\n",
    "import boltzgen.mixed as mixed\n",
    "\n",
    "import normflow as nf\n",
    "from boltzgen.flows import CoordinateTransform\n",
    "from boltzgen.distributions import Boltzmann\n",
    "\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from autograd import grad\n",
    "from autograd import numpy as np\n",
    "from openmmtools.constants import kB\n",
    "from simtk import openmm as mm\n",
    "from simtk import unit\n",
    "from simtk.openmm import app\n",
    "from openmmtools.testsystems import AlanineDipeptideImplicit\n",
    "\n",
    "# Load the alanine dipeptide trajectory\n",
    "aldp_traj = mdtraj.load('/scratch2/vs488/flow/alanine_dipeptide/trajectory/aldp100000.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up coordinate transformation\n",
    "\n",
    "z_matrix = [\n",
    "    (1, [4, 5, 6]),\n",
    "    (0, [1, 4, 5]),\n",
    "    (2, [1, 0, 4]),\n",
    "    (3, [1, 0, 2]),\n",
    "    (7, [6, 4, 5]),\n",
    "    (9, [8, 6, 7]),\n",
    "    (10, [8, 6, 9]),\n",
    "    (11, [10, 8, 9]),\n",
    "    (12, [10, 8, 11]),\n",
    "    (13, [10, 11, 12]),\n",
    "    (17, [16, 14, 15]),\n",
    "    (19, [18, 16, 17]),\n",
    "    (20, [18, 19, 16]),\n",
    "    (21, [18, 19, 20])\n",
    "]\n",
    "\n",
    "backbone_indices = [4, 5, 6, 8, 14, 15, 16, 18]\n",
    "\n",
    "aldp_traj.center_coordinates()\n",
    "\n",
    "# superpose on the backbone\n",
    "ind = aldp_traj.top.select(\"backbone\")\n",
    "\n",
    "aldp_traj.superpose(aldp_traj, 0, atom_indices=ind, ref_atom_indices=ind)\n",
    "\n",
    "# Gather the training data into a pytorch Tensor with the right shape\n",
    "training_data = aldp_traj.xyz\n",
    "n_atoms = training_data.shape[1]\n",
    "n_dim = n_atoms * 3\n",
    "training_data_npy = training_data.reshape(-1, n_dim)\n",
    "training_data = torch.from_numpy(training_data_npy.astype(\"float64\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up simulation object for energy computation\n",
    "\n",
    "temperature = 1000\n",
    "kT = kB * temperature\n",
    "\n",
    "testsystem = AlanineDipeptideImplicit()\n",
    "implicit_sim = app.Simulation(testsystem.topology,\n",
    "                              testsystem.system,\n",
    "                              mm.LangevinIntegrator(temperature * unit.kelvin , 1.0 / unit.picosecond, 1.0 * unit.femtosecond),\n",
    "                              platform=mm.Platform.getPlatformByName('CPU')\n",
    "                              )\n",
    "implicit_sim.context.setPositions(testsystem.positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up flow model\n",
    "\n",
    "# Define flows\n",
    "K = 8\n",
    "torch.manual_seed(0)\n",
    "\n",
    "latent_size = 60\n",
    "b = torch.Tensor([1 if i % 2 == 0 else 0 for i in range(latent_size)])\n",
    "flows = []\n",
    "for i in range(K):\n",
    "    s = nf.nets.MLP([latent_size, 4 * latent_size, 4 * latent_size, latent_size], output_fn='tanh', output_scale=3.)\n",
    "    t = nf.nets.MLP([latent_size, 4 * latent_size, 4 * latent_size, latent_size], output_fn='tanh', output_scale=3.)\n",
    "    if i % 2 == 0:\n",
    "        flows += [nf.flows.MaskedAffineFlow(b, s, t)]\n",
    "    else:\n",
    "        flows += [nf.flows.MaskedAffineFlow(1 - b, s, t)]\n",
    "    #flows += [nf.flows.Planar(latent_size)]\n",
    "    flows += [nf.flows.ActNorm(latent_size)]\n",
    "flows += [CoordinateTransform(training_data, 66, z_matrix, backbone_indices)]\n",
    "\n",
    "# Set prior and q0\n",
    "p = Boltzmann(implicit_sim.context, temperature, energy_cut=1e10, energy_max=1e20)\n",
    "q0 = nf.distributions.DiagGaussian(latent_size)\n",
    "\n",
    "# Construct flow model\n",
    "nfm = nf.NormalizingFlow(q0=q0, flows=flows, p=p)\n",
    "\n",
    "# Move model on GPU if available\n",
    "enable_cuda = False\n",
    "device = torch.device('cuda' if torch.cuda.is_available() and enable_cuda else 'cpu')\n",
    "nfm = nfm.to(device)\n",
    "nfm = nfm.double()\n",
    "\n",
    "ind = torch.randint(len(training_data), (128, ))\n",
    "x = training_data[ind, :].double()\n",
    "kld = nfm.forward_kld(x)\n",
    "#kld = nfm.reverse_kld(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "batch_size = 128\n",
    "num_samples = 64\n",
    "max_iter = 500\n",
    "trans_iter = 8000\n",
    "n_data = len(training_data)\n",
    "eval_rkld = 10\n",
    "\n",
    "\n",
    "loss_hist = np.array([])\n",
    "fkld_hist = np.array([])\n",
    "rkld_hist = np.array([])\n",
    "\n",
    "optimizer = torch.optim.AdamW(nfm.parameters(), lr=1e-4, weight_decay=1e-3)\n",
    "for it in tqdm(range(max_iter)):\n",
    "    #nfm.p.alpha = np.max([0., 1 - it / trans_iter])\n",
    "    optimizer.zero_grad()\n",
    "    ind = torch.randint(n_data, (batch_size, ))\n",
    "    x = training_data[ind, :].double()\n",
    "    fkld = nfm.forward_kld(x)\n",
    "    rkld = nfm.reverse_kld(num_samples=num_samples)\n",
    "    loss = fkld + rkld\n",
    "    if not torch.isnan(loss):\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_value_(nfm.parameters(), .01)\n",
    "        gradient_norm = torch.nn.utils.clip_grad.clip_grad_norm_(nfm.parameters(), 100.)\n",
    "        optimizer.step()\n",
    "    \n",
    "    loss_hist = np.append(loss_hist, loss.to('cpu').data.numpy())\n",
    "    fkld_hist = np.append(fkld_hist, fkld.to('cpu').data.numpy())\n",
    "    rkld_hist = np.append(rkld_hist, rkld.to('cpu').data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train model with complex schedule\n",
    "batch_size = [32, 64] + 8 * [128]\n",
    "num_iter = 3 * [1500] + 6 * [30] + [300]\n",
    "num_samples = 3 * [64] + 7 * [512]\n",
    "lr = 3 * [1e-3] + 7 * [1e-4]\n",
    "E_cut = 4 * [1e-10] + [1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-5]\n",
    "w_kl = 3 * [0] + [1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-2]\n",
    "\n",
    "n_data = len(training_data)\n",
    "\n",
    "loss_hist = np.array([])\n",
    "fkld_hist = np.array([])\n",
    "rkld_hist = np.array([])\n",
    "\n",
    "optimizer = torch.optim.AdamW(nfm.parameters(), lr=lr[0], weight_decay=1e-4)\n",
    "\n",
    "for i in range(len(num_iter)):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr[i]\n",
    "    nfm.p = Boltzmann(implicit_sim.context, temperature, energy_cut=E_cut[i], energy_max=1e20)\n",
    "    for it in tqdm(range(num_iter[i])):\n",
    "        optimizer.zero_grad()\n",
    "        ind = torch.randint(n_data, (batch_size[i], ))\n",
    "        x = training_data[ind, :]\n",
    "        #with torch.autograd.detect_anomaly():\n",
    "        fkld = nfm.forward_kld(x)\n",
    "        rkld = nfm.reverse_kld(num_samples=num_samples[i])\n",
    "        if w_kl[i] > 0:\n",
    "            loss = fkld + w_kl[i] * rkld\n",
    "        else:\n",
    "            loss = fkld\n",
    "        if not torch.isnan(loss):\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_value_(nfm.parameters(), .01)\n",
    "            gradient_norm = torch.nn.utils.clip_grad.clip_grad_norm_(nfm.parameters(), 100.)\n",
    "            #print(gradient_norm)\n",
    "            optimizer.step()\n",
    "            #print(nfm.flows[-2].s[0, nfm.flows[-1].mixed_transform.ic_transform.rev_z_indices[:, 0]])\n",
    "            #print(nfm.flows[-2].t[0, nfm.flows[-1].mixed_transform.ic_transform.rev_z_indices[:, 0]])\n",
    "        #print(nfm.flows[0].s.net[0].weight._grad)\n",
    "        #print(nfm.flows[0].s.net[0].weight)\n",
    "\n",
    "        loss_hist = np.append(loss_hist, loss.to('cpu').data.numpy())\n",
    "        fkld_hist = np.append(fkld_hist, fkld.to('cpu').data.numpy())\n",
    "        rkld_hist = np.append(rkld_hist, rkld.to('cpu').data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_hist[loss_hist > 0] = np.nan\n",
    "plt.plot(loss_hist)\n",
    "plt.show()\n",
    "plt.plot(fkld_hist)\n",
    "plt.show()\n",
    "plt.plot(rkld_hist)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x, _ = nfm.sample(32)\n",
    "print(nfm.p.log_prob(x))\n",
    "x = training_data[ind, :].double()\n",
    "print(nfm.p.log_prob(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z, log_q = nfm.q0(8)\n",
    "for flow in nfm.flows:\n",
    "    z, log_det = flow(z)\n",
    "    print(torch.norm(z, dim=1))\n",
    "    print(log_det)\n",
    "    log_q -= log_det"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfm.flows[-1].mixed_transform.ic_transform.bond_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z, _ = nfm.sample(100000)\n",
    "z, _ = nfm.flows[-1].inverse(z)\n",
    "z_d, _ = nfm.flows[-1].inverse(training_data.double())\n",
    "z_np = z.data.numpy()\n",
    "z_d_np = z_d.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(60):\n",
    "    print(i)\n",
    "    plt.hist(z_d_np[:, i], bins=100, alpha=0.5, label='data', range=[-3.1, 3.1])\n",
    "    plt.hist(z_np[:, i], bins=100, alpha=0.5, label='samples', range=[-3.1, 3.1])\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
