{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitpytorch2conda2d47d61a0f134d8aac9f1e53c6140db5",
   "display_name": "Python 3.7.6 64-bit ('pytorch2': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import normflow as nf\n",
    "import mdtraj\n",
    "sys.path.append(\"../\")\n",
    "import boltzgen.mixed as mixed\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need a more generic flow as we don't have a target dist\n",
    "# and won't be using the KL loss\n",
    "class SNFlow(nn.Module):\n",
    "    def __init__(self, q0, flows):\n",
    "        super().__init__()\n",
    "        self.flows = nn.ModuleList(flows)\n",
    "        self.q0 = q0\n",
    "\n",
    "    def forward(self, num_samples):\n",
    "        # Draw some samples from q0 and pass them forward through the flow\n",
    "        dummy_var = torch.zeros((num_samples,))\n",
    "        x, _ = self.q0(dummy_var, num_samples=1)\n",
    "        x = x.view((x.shape[0], x.shape[2]))\n",
    "        totlogdet = torch.zeros((x.shape[0],))\n",
    "        for flow in self.flows:\n",
    "            x, logdet = flow(x)\n",
    "            totlogdet += logdet\n",
    "\n",
    "        return x, totlogdet\n",
    "\n",
    "    def inverse(self, x):\n",
    "        # Pass the given x backward through the flow\n",
    "        tot_invlogdet = torch.zeros((x.shape[0],))\n",
    "\n",
    "        for i in range(len(self.flows)-1, -1, -1):\n",
    "            x, invlogdet = self.flows[i].inverse(x)\n",
    "            tot_invlogdet += invlogdet\n",
    "\n",
    "        return x, tot_invlogdet\n",
    "\n",
    "    def dih_quad_loss(self, batch_size):\n",
    "        # Quadratic loss on dihedral angle dimensions only applied\n",
    "        # outside the range +/- pi.\n",
    "        # As described in the boltzmann generators supplementary materials\n",
    "        x, _ = self.forward(batch_size)\n",
    "        # dih_indeces are the dimensions corresponding to the dihedral angles\n",
    "        dih_indeces = [20, 23, 26, 29, 32, 35, 38, 41, 44, 47, 50, 53, 56, 59]\n",
    "        dih_x = x[:, dih_indeces]\n",
    "        vio_dih_x = torch.where(torch.abs(dih_x)>3.1416, dih_x,\n",
    "            torch.tensor(0).double())\n",
    "        loss = torch.mean(torch.sum(vio_dih_x ** 2, dim=-1))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If not done already, generate the 10^5 training samples\n",
    "\n",
    "import openmmtools\n",
    "from openmmtools.constants import kB\n",
    "from simtk import openmm as mm\n",
    "from simtk import unit\n",
    "from simtk.openmm import app\n",
    "from openmmtools.testsystems import AlanineDipeptideVacuum\n",
    "import mdtraj\n",
    "from sys import stdout\n",
    "from simtk.openmm.app import *\n",
    "\n",
    "temperature = 1000\n",
    "kT = kB * temperature\n",
    "testsystem = AlanineDipeptideVacuum()\n",
    "vacuum_sim = app.Simulation(testsystem.topology,\n",
    "                            testsystem.system,\n",
    "                            mm.LangevinIntegrator(temperature * unit.kelvin , 1.0 / unit.picosecond, 1.0 * unit.femtosecond),\n",
    "                            platform=mm.Platform.getPlatformByName('CPU')\n",
    "                            )\n",
    "vacuum_sim.context.setPositions(testsystem.positions)\n",
    "vacuum_sim.minimizeEnergy()\n",
    "vacuum_sim.reporters.append(mdtraj.reporters.HDF5Reporter('aldp_training_data.h5', 10))\n",
    "vacuum_sim.reporters.append(StateDataReporter(stdout, 100000, step=True,\n",
    "        potentialEnergy=True, temperature=True))\n",
    "vacuum_sim.step(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data \n",
    "aldp_traj = mdtraj.load('aldp_training_data.h5')\n",
    "z = [\n",
    "    (1, [4, 5, 6]),\n",
    "    (0, [1, 4, 5]),\n",
    "    (2, [1, 0, 4]),\n",
    "    (3, [1, 0, 2]),\n",
    "    (7, [6, 4, 5]),\n",
    "    (9, [8, 6, 5]),\n",
    "    (10, [8, 6, 9]),\n",
    "    (11, [10, 8, 5]),\n",
    "    (12, [10, 8, 11]),\n",
    "    (13, [10, 11, 12]),\n",
    "    (17, [16, 14, 15]),\n",
    "    (19, [18, 16, 17]),\n",
    "    (20, [18, 19, 16]),\n",
    "    (21, [18, 19, 20])\n",
    "]\n",
    "backbone_indices = [4, 5, 6, 8, 14, 15, 16, 18]\n",
    "aldp_traj.center_coordinates()\n",
    "ind = aldp_traj.top.select(\"backbone\")\n",
    "aldp_traj.superpose(aldp_traj, 0, atom_indices=ind, ref_atom_indices=ind)\n",
    "\n",
    "training_data = aldp_traj.xyz\n",
    "n_atoms = training_data.shape[1]\n",
    "n_dim = n_atoms * 3\n",
    "training_data_npy = training_data.reshape(-1, n_dim)\n",
    "training_data = torch.from_numpy(training_data_npy)\n",
    "training_data = training_data.double()\n",
    "\n",
    "# We can pre-apply the mixed transform to all the training data\n",
    "# and then fit the flow to the transformed data meaning\n",
    "# that we don't need to put the transform at the end of the\n",
    "# flow\n",
    "\n",
    "mixed_transform = mixed.MixedTransform(66, backbone_indices, z, training_data)\n",
    "x, _ = mixed_transform.forward(training_data)\n",
    "print(\"Training data shape\", x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------ Set up model -----------\n",
    "\n",
    "# Define flows\n",
    "K = 6\n",
    "#torch.manual_seed(0)\n",
    "\n",
    "# If we use the the 'velocity' dimensions as mentioned\n",
    "# in the stochastic normalizing flows paper we need\n",
    "# to double the dimensionality of the flow.\n",
    "# Since we are fitting the flow to the transformed\n",
    "# training data there are 60 dimensions since the\n",
    "# original data has 66 dims and 6 dims are discarded\n",
    "# in the transform\n",
    "flow_dim = 120\n",
    "b = torch.zeros((flow_dim,))\n",
    "b[int(flow_dim/2):flow_dim] = 1\n",
    "flows = []\n",
    "for i in range(K):\n",
    "    s = nf.nets.MLP([flow_dim, 128, 128, 128, flow_dim])\n",
    "    t = nf.nets.MLP([flow_dim, 128, 128, 128, flow_dim])\n",
    "    if i % 2 == 0:\n",
    "        flows += [nf.flows.MaskedAffineFlow(b, s, t)]\n",
    "    else:\n",
    "        flows += [nf.flows.MaskedAffineFlow(1 - b, s, t)]\n",
    "\n",
    "q0 = nf.distributions.ConstDiagGaussian(np.zeros(flow_dim, dtype=np.float32), np.ones(flow_dim, dtype=np.float32))\n",
    "\n",
    "# Construct flow model\n",
    "nfm = SNFlow(q0=q0, flows=flows)\n",
    "\n",
    "# Move model on GPU if available\n",
    "enable_cuda = True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() and enable_cuda else 'cpu')\n",
    "print(\"Device: \", device)\n",
    "nfm = nfm.to(device)\n",
    "nfm = nfm.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "mini_batch_size = 256\n",
    "\n",
    "optimizer = torch.optim.Adam(nfm.parameters(), lr=1e-6, weight_decay=1e-3)\n",
    "for epoch_idx in range(num_epochs):\n",
    "    shuffled_ids = torch.randperm(x.shape[0])\n",
    "    epoch_losses = []\n",
    "    for batch_idx in range(0, x.shape[0], mini_batch_size):\n",
    "        mini_batch_ids = shuffled_ids[batch_idx:batch_idx+mini_batch_size]\n",
    "        mini_batch = x[mini_batch_ids]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        mini_batch = mini_batch.double()\n",
    "        # augment the training sample with 60 'velocities'\n",
    "        vels = torch.randn_like(mini_batch)\n",
    "        y = torch.cat([mini_batch, vels], 1)\n",
    "        y0, invlogdet = nfm.inverse(y)\n",
    "        y02 = torch.matmul(y0.view((y0.shape[0], 1, y0.shape[1])),\n",
    "            y0.view((y0.shape[0], y0.shape[1], 1))).view((y0.shape[0],))\n",
    "        Jml = y02.mean() - invlogdet.mean()\n",
    "        dih_loss = nfm.dih_quad_loss(mini_batch_size)\n",
    "        dual_loss = Jml + dih_loss\n",
    "\n",
    "        dual_loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_losses.append([Jml.cpu().data.numpy(), dih_loss.cpu().data.numpy()])\n",
    "\n",
    "    epoch_losses = np.array(epoch_losses)\n",
    "    print(\"Jml\", np.mean(epoch_losses[:, 0]), \"dih\", np.mean(epoch_losses[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples,_ = nfm.forward(10000)\n",
    "samples = samples.cpu().data.numpy()\n",
    "samples = samples[:, :60]\n",
    "print(samples.shape)\n",
    "\n",
    "import scipy.stats\n",
    "\n",
    "# some training distributions seem flat as they\n",
    "# are highly concentrated due to the normalisation\n",
    "# failing when the std is very small\n",
    "\n",
    "for i in range(60):\n",
    "    print(i)\n",
    "    kde_data = scipy.stats.gaussian_kde(x[:, i])\n",
    "    kde_flow = scipy.stats.gaussian_kde(samples[:, i])\n",
    "    positions = np.linspace(-10, 10, 100)\n",
    "    plt.plot(positions, kde_data(positions))\n",
    "    plt.plot(positions, kde_flow(positions))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}